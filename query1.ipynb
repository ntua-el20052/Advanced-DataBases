{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea13464-ee26-43b2-a66c-7d3ac13315db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1833</td><td>application_1732639283265_1794</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1794/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1794_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|   age_group| count|\n",
      "+------------+------+\n",
      "|      Adults|121052|\n",
      "|Young adults| 33588|\n",
      "|    Children| 10825|\n",
      "|     Elderly|  5985|\n",
      "+------------+------+\n",
      "\n",
      "Execution time for DataFrame API: 5.672727584838867 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Initialize a Spark session with specified configurations\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Query_1\") \\\n",
    "        .config(\"spark.executor.instances\", 4) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Load the dataset from an S3 bucket\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/\")\n",
    "\n",
    "# Start the timer to measure execution time\n",
    "start_time_df = time.time()\n",
    "\n",
    "# Filter out rows with invalid latitude and longitude values (LAT and LON must not be 0)\n",
    "filtered_df = df.filter((col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "\n",
    "# Filter the dataset to include only rows where the crime description contains \"AGGRAVATED ASSAULT\"\n",
    "df_filtered = filtered_df.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\"))\n",
    "\n",
    "# Categorize victims into age groups using the \"Vict Age\" column\n",
    "df_with_age_groups = df_filtered.withColumn(\n",
    "    \"age_group\",\n",
    "    when((col(\"Vict Age\").cast(\"int\") > 0) & (col(\"Vict Age\").cast(\"int\") < 18), \"Children\")  # Ages 0-17\n",
    "    .when((col(\"Vict Age\").cast(\"int\") >= 18) & (col(\"Vict Age\").cast(\"int\") <= 24), \"Young adults\")  # Ages 18-24\n",
    "    .when((col(\"Vict Age\").cast(\"int\") >= 25) & (col(\"Vict Age\").cast(\"int\") <= 64), \"Adults\")  # Ages 25-64\n",
    "    .when(col(\"Vict Age\").cast(\"int\") > 64, \"Elderly\")  # Age 65+\n",
    ")\n",
    "\n",
    "# Group by age group and count the number of occurrences for each group\n",
    "age_group_counts = df_with_age_groups.groupBy(\"age_group\").count()\n",
    "\n",
    "# Order the results by the count in descending order\n",
    "result_df = age_group_counts.orderBy(col(\"count\").desc())\n",
    "\n",
    "# Filter out rows with null age groups and display the results\n",
    "result_df.filter(result_df[\"age_group\"].isNotNull()).show()\n",
    "\n",
    "# Calculate and print the total execution time for the DataFrame operations\n",
    "execution_time_df = time.time() - start_time_df\n",
    "print(f\"Execution time for DataFrame API: {execution_time_df} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c96b23-522e-4304-b752-9404e6f46c00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Group Counts:\n",
      "Adults: 121052\n",
      "Young adults: 33588\n",
      "Children: 10825\n",
      "Elderly: 5985\n",
      "Execution time for RDD API: 24.93943738937378 seconds"
     ]
    }
   ],
   "source": [
    "# Load the data into an RDD from the S3 bucket\n",
    "data_rdd = spark.sparkContext.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/\")\n",
    "\n",
    "# Start the timer to measure execution time\n",
    "start_time_rdd = time.time()\n",
    "\n",
    "# Extract the first row (header) from the RDD\n",
    "header = data_rdd.first()\n",
    "\n",
    "# Filter out the header from the data RDD\n",
    "data_rdd = data_rdd.filter(lambda row: row != header)\n",
    "\n",
    "# Function to split each row into columns (using csv.reader to handle commas properly)\n",
    "def split_row(row):\n",
    "    import csv\n",
    "    from io import StringIO\n",
    "    return list(csv.reader(StringIO(row)))[0]\n",
    "\n",
    "# Apply the split_row function to convert rows into columns\n",
    "data_rdd = data_rdd.map(split_row)\n",
    "\n",
    "# Extract column names from the header to find the indices for relevant columns\n",
    "columns = header.split(\",\")\n",
    "crm_cd_desc_idx = columns.index(\"Crm Cd Desc\")\n",
    "vict_age_idx = columns.index(\"Vict Age\")\n",
    "lat_idx = columns.index(\"LAT\")\n",
    "lon_idx = columns.index(\"LON\")\n",
    "\n",
    "# Filter the data RDD for aggravated assault crimes, and remove rows with invalid LAT/LON\n",
    "filtered_rdd = data_rdd.filter(lambda row: \n",
    "    \"AGGRAVATED ASSAULT\" in row[crm_cd_desc_idx] and \n",
    "    (row[lat_idx] != '0' and row[lon_idx] != '0')\n",
    ")\n",
    "\n",
    "# Function to categorize the victims based on age and return a tuple for counting\n",
    "def categorize_and_count(row):\n",
    "    try:\n",
    "        age = int(row[vict_age_idx])\n",
    "        if 0 < age < 18:\n",
    "            age_group = \"Children\"\n",
    "        elif 18 <= age <= 24:\n",
    "            age_group = \"Young adults\"\n",
    "        elif 25 <= age <= 64:\n",
    "            age_group = \"Adults\"\n",
    "        elif age > 64:\n",
    "            age_group = \"Elderly\"\n",
    "        else:\n",
    "            age_group = \"Unknown\"\n",
    "    except ValueError:  # Handle cases where the age is not a valid integer\n",
    "        age_group = \"Unknown\"\n",
    "    return (age_group, 1)\n",
    "\n",
    "# Categorize and count the occurrences of each age group, then sort by count in descending order\n",
    "age_group_counts_rdd = filtered_rdd.map(categorize_and_count) \\\n",
    "                                 .reduceByKey(lambda a, b: a + b) \\\n",
    "                                 .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Filter out the 'Unknown' age group\n",
    "filtered_age_group_counts_rdd = age_group_counts_rdd.filter(lambda x: x[0] != \"Unknown\")\n",
    "\n",
    "# Collect the results to the driver\n",
    "results = filtered_age_group_counts_rdd.collect()\n",
    "\n",
    "# Print the results\n",
    "print(\"Age Group Counts:\")\n",
    "for age_group, count in results:\n",
    "    print(f\"{age_group}: {count}\")\n",
    "\n",
    "# Calculate and print the execution time for the RDD operations\n",
    "execution_time_rdd = time.time() - start_time_rdd\n",
    "print(f\"Execution time for RDD API: {execution_time_rdd} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e3b265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287cb29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
