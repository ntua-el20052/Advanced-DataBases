{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea13464-ee26-43b2-a66c-7d3ac13315db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3141</td><td>application_1732639283265_3097</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3097/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-247.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3097_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+-------------------------+\n",
      "|         area|       crime_ratio|average_income_per_person|\n",
      "+-------------+------------------+-------------------------+\n",
      "|       Vernon| 6.339285714285714|        4406.446428571428|\n",
      "|     Downtown| 4.208144796380091|       19107.378746594004|\n",
      "| Little Tokyo| 4.020673360897814|       23815.344594594593|\n",
      "|    Hollywood|1.5511440107671601|        25648.05024674742|\n",
      "|    Chinatown|1.2563501367721766|        14058.45616777387|\n",
      "|  Rancho Park|1.2552819698173154|        38740.06417791898|\n",
      "|       Venice|1.2549272030651342|         45826.1724004093|\n",
      "| Leimert Park|1.2148193014334747|       17064.604636118598|\n",
      "|Baldwin Hills|1.1940496560393896|       16899.742774277427|\n",
      "|  Westchester|1.1896619947102067|       30705.685088633993|\n",
      "|    Hyde Park| 1.083026160397953|       14103.120570456438|\n",
      "| Park La Brea| 1.025207944321847|        36619.89882872178|\n",
      "|     Westlake|1.0028551557524068|         10639.3003783415|\n",
      "|        Watts|0.9950576190836422|         7754.24222709736|\n",
      "|  Studio City|0.9295754238516157|        44635.81627719581|\n",
      "|     Van Nuys|0.9170415838361292|       16156.408411451735|\n",
      "|      Melrose| 0.909787014117455|        38249.46119328569|\n",
      "|    Los Feliz|0.9076272010896002|       29484.927604605447|\n",
      "| Hancock Park|0.8983094426946069|        16654.56343039041|\n",
      "|  Canoga Park|0.8960521181480413|        18707.82648450001|\n",
      "+-------------+------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time for DataFrame API: 79.52657318115234 seconds"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, expr, count, regexp_replace, collect_list, first, trim\n",
    "import time\n",
    "\n",
    "# Initialize a Spark session with Sedona support for geospatial operations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query_3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize Sedona context and register geospatial functions\n",
    "sedona = SedonaContext.create(spark)\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Load GeoJSON data for population and census blocks\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "population_df_1 = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "income_df_1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\")\n",
    "\n",
    "# Start a timer to measure execution time\n",
    "start_time_df = time.time()\n",
    "\n",
    "# Extract relevant columns from population GeoJSON and cast them to the appropriate types\n",
    "population_df = population_df_1.select(\n",
    "    col(\"properties.COMM\").alias(\"area\"),\n",
    "    col(\"properties.POP_2010\").cast(\"float\").alias(\"population\"),\n",
    "    col(\"properties.ZCTA10\").cast(\"float\").alias(\"zip\"),\n",
    "    col(\"geometry\").alias(\"geometry1\"),\n",
    "    col(\"properties.HOUSING10\").alias(\"households\")\n",
    ")\n",
    "\n",
    "\n",
    "# Aggregate population and household data by area and zip\n",
    "population_aggregated_df = population_df.groupBy(\"area\", \"zip\").agg(\n",
    "    sum(\"population\").cast(\"float\").alias(\"total_population\"),\n",
    "    sum(\"households\").cast(\"float\").alias(\"total_housholds\"),\n",
    "    collect_list(\"geometry1\").alias(\"geometry_array\")\n",
    ")\n",
    "\n",
    "# Perform a geometric union of all geometries within each area and zip\n",
    "population_geometry_combined = population_aggregated_df.select(\n",
    "    \"area\",\n",
    "    \"zip\",\n",
    "    \"total_population\",\n",
    "    ST_Union(\"geometry_array\").alias(\"combined_geometry\")\n",
    ")\n",
    "\n",
    "# Process income data: clean up income field and cast it to float\n",
    "income_df = income_df_1.select(\n",
    "    col(\"Community\").alias(\"area\"),\n",
    "    col(\"Zip Code\").alias(\"zip\"),\n",
    "    regexp_replace(col(\"Estimated Median Income\"), r'[\\$,]', '').cast(\"float\").alias(\"household_income\")\n",
    ")\n",
    "\n",
    "# Join population data with income data based on area and zip\n",
    "joined1_df = population_aggregated_df.join(\n",
    "    income_df,\n",
    "    (income_df[\"area\"].contains(population_aggregated_df[\"area\"])) &\n",
    "    (population_aggregated_df[\"zip\"] == income_df[\"zip\"]),\n",
    "    how=\"inner\").drop(income_df[\"area\"])\n",
    "\n",
    "# Ensure columns are properly cast for further calculations\n",
    "joined1_df = joined1_df.withColumn(\n",
    "    \"total_population\", col(\"total_population\").cast(\"float\")\n",
    ").withColumn(\n",
    "    \"household_income\", col(\"household_income\").cast(\"float\")\n",
    ").withColumn(\n",
    "    \"total_housholds\", col(\"total_housholds\").cast(\"float\")\n",
    ")\n",
    "\n",
    "# Calculate total household income and total population for each area\n",
    "area_aggregated_df = joined1_df.groupBy(\"area\").agg(\n",
    "    sum(col(\"total_housholds\") * col(\"household_income\")).cast(\"float\").alias(\"total_housholds_income\"),\n",
    "    sum(\"total_population\").cast(\"float\").alias(\"total_population_sum\")\n",
    ")\n",
    "\n",
    "# Compute average income per person in each area\n",
    "result1_df = area_aggregated_df.withColumn(\n",
    "    \"average_income_per_person\",\n",
    "    (col(\"total_housholds_income\")) / col(\"total_population_sum\")\n",
    ")\n",
    "\n",
    "# Load crime data and filter invalid geographical entries\n",
    "crime_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/\")\n",
    "crime_df = crime_df.filter((col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "crime_df = crime_df.withColumn(\"geography\", ST_Point(\"LON\", \"LAT\"))\n",
    "crime_df = crime_df.select(col(\"geography\"))\n",
    "\n",
    "# Spatial join between crimes and combined population geometries\n",
    "joined2_df = crime_df.join(\n",
    "    population_geometry_combined, ST_Within(crime_df.geography, population_geometry_combined.combined_geometry)\n",
    ")\n",
    "\n",
    "# Aggregate crime counts and total population for each area and zip\n",
    "joined2_df = joined2_df.groupBy(\"area\", \"zip\").agg(\n",
    "    count(\"*\").cast(\"float\").alias(\"total_crimes\"),\n",
    "    first(\"total_population\").cast(\"float\").alias(\"total_population\")\n",
    ")\n",
    "\n",
    "# Aggregate crime data by area\n",
    "joined2_df = joined2_df.groupBy(\"area\").agg(\n",
    "    sum(col(\"total_crimes\")).cast(\"float\").alias(\"total_population_crime\"),\n",
    "    sum(\"total_population\").cast(\"float\").alias(\"total_population_sum\")\n",
    ")\n",
    "\n",
    "# Calculate crime ratio for each area\n",
    "result2_df = joined2_df.withColumn(\n",
    "    \"crime_ratio\",\n",
    "    (col(\"total_population_crime\")) / col(\"total_population_sum\")\n",
    ")\n",
    "\n",
    "# Combine crime data with average income data\n",
    "final_df = result2_df.join(\n",
    "    result1_df.select(\"area\", \"average_income_per_person\"),\n",
    "    on=\"area\",\n",
    "    how=\"inner\"\n",
    ").select(\"area\", \"crime_ratio\", \"average_income_per_person\")\n",
    "\n",
    "# Filter out rows with null or empty area values\n",
    "final_df = final_df.filter(final_df[\"area\"].isNotNull() & (trim(final_df[\"area\"]) != \"\"))\n",
    "\n",
    "# Display final results\n",
    "final_df.orderBy(col(\"crime_ratio\").desc()).show()\n",
    "\n",
    "# Measure execution time for the entire DataFrame API process\n",
    "execution_time_df = time.time() - start_time_df\n",
    "print(f\"Execution time for DataFrame API: {execution_time_df} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
