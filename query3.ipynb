{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea13464-ee26-43b2-a66c-7d3ac13315db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1829</td><td>application_1732639283265_1790</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1790/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1790_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-------------------------+\n",
      "|            area|         crime_ratio|average_income_per_person|\n",
      "+----------------+--------------------+-------------------------+\n",
      "|     Culver City| 0.03574827045238279|        33649.85826366559|\n",
      "|  Toluca Terrace| 0.23059185242121444|       20167.532667179094|\n",
      "|     Pico Rivera|3.178892156083604...|       15155.647588015576|\n",
      "|          Malibu|7.908889591901297E-5|         67132.4517557735|\n",
      "|      Montebello|9.612457745237828E-5|        14515.23568144315|\n",
      "| Lincoln Heights|  0.6226239404058567|       10512.428664778916|\n",
      "|Westlake Village|1.417434443656980...|        42843.46118500605|\n",
      "|        Van Nuys|  0.9170415838361292|       16156.408411451735|\n",
      "|          Carson|0.004699391586889679|        20425.47903876513|\n",
      "| Rowland Heights|4.405868616997841E-5|        17894.44173238754|\n",
      "|    Agoura Hills|2.951303492375799...|        42688.28962124939|\n",
      "|        Glendale|9.560296650381357E-4|        23173.42104123194|\n",
      "|    Hidden Hills|0.001616379310344...|        38682.48275862069|\n",
      "|   Boyle Heights|  0.7219916008084525|        8195.271917274851|\n",
      "|   Granada Hills|  0.5782462118465889|        26881.56993275886|\n",
      "|     North Hills|  0.6554900777450389|       16051.707697867727|\n",
      "|      Northridge|  0.7793910934676161|       23168.415890969125|\n",
      "|     Signal Hill|1.816530426884650...|       26272.191825613078|\n",
      "| Wilshire Center|  0.8635796595834655|        16168.32855577471|\n",
      "|        Glendora|3.900764549851771E-5|       27093.313133704185|\n",
      "+----------------+--------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time for DataFrame API: 57.487706661224365 seconds"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, expr, count, regexp_replace, collect_list, first, trim\n",
    "import time\n",
    "\n",
    "# Initialize a Spark session with Sedona support for geospatial operations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query_3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize Sedona context and register geospatial functions\n",
    "sedona = SedonaContext.create(spark)\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Load GeoJSON data for population and census blocks\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "population_df_1 = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "# Load income data as a CSV file\n",
    "income_df_1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\")\n",
    "\n",
    "# Start a timer to measure execution time\n",
    "start_time_df = time.time()\n",
    "\n",
    "# Extract relevant columns from population GeoJSON and cast them to the appropriate types\n",
    "population_df = population_df_1.select(\n",
    "    col(\"properties.COMM\").alias(\"area\"),\n",
    "    col(\"properties.POP_2010\").cast(\"float\").alias(\"population\"),\n",
    "    col(\"properties.ZCTA10\").cast(\"float\").alias(\"zip\"),\n",
    "    col(\"geometry\").alias(\"geometry1\"),\n",
    "    col(\"properties.HOUSING10\").alias(\"households\")\n",
    ")\n",
    "\n",
    "# Aggregate population and household data by area and zip\n",
    "population_aggregated_df = population_df.groupBy(\"area\", \"zip\").agg(\n",
    "    sum(\"population\").cast(\"float\").alias(\"total_population\"),\n",
    "    sum(\"households\").cast(\"float\").alias(\"total_housholds\")\n",
    ")\n",
    "\n",
    "# Collect geometries into an array for each area and zip\n",
    "population_geometry_array = population_df.groupBy(\"area\", \"zip\").agg(\n",
    "    collect_list(\"geometry1\").alias(\"geometry_array\")\n",
    ")\n",
    "\n",
    "# Combine aggregated population data with geometries\n",
    "population_geometry_combined_input = population_aggregated_df.join(\n",
    "    population_geometry_array,\n",
    "    on=[\"area\", \"zip\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Perform a geometric union of all geometries within each area and zip\n",
    "population_geometry_combined = population_geometry_combined_input.select(\n",
    "    \"area\",\n",
    "    \"zip\",\n",
    "    \"total_population\",\n",
    "    ST_Union(\"geometry_array\").alias(\"combined_geometry\")\n",
    ")\n",
    "\n",
    "# Process income data: clean up income field and cast it to float\n",
    "income_df = income_df_1.select(\n",
    "    col(\"Community\").alias(\"area\"),\n",
    "    col(\"Zip Code\").alias(\"zip\"),\n",
    "    regexp_replace(col(\"Estimated Median Income\"), r'[\\$,]', '').cast(\"float\").alias(\"household_income\")\n",
    ")\n",
    "\n",
    "# Join population data with income data based on area and zip\n",
    "joined1_df = population_aggregated_df.join(\n",
    "    income_df,\n",
    "    (income_df[\"area\"].contains(population_aggregated_df[\"area\"])) &\n",
    "    (population_aggregated_df[\"zip\"] == income_df[\"zip\"]),\n",
    "    how=\"inner\").drop(income_df[\"area\"])\n",
    "\n",
    "# Ensure columns are properly cast for further calculations\n",
    "joined1_df = joined1_df.withColumn(\n",
    "    \"total_population\", col(\"total_population\").cast(\"float\")\n",
    ").withColumn(\n",
    "    \"household_income\", col(\"household_income\").cast(\"float\")\n",
    ").withColumn(\n",
    "    \"total_housholds\", col(\"total_housholds\").cast(\"float\")\n",
    ")\n",
    "\n",
    "# Calculate total household income and total population for each area\n",
    "area_aggregated_df = joined1_df.groupBy(\"area\").agg(\n",
    "    sum(col(\"total_housholds\") * col(\"household_income\")).cast(\"float\").alias(\"total_housholds_income\"),\n",
    "    sum(\"total_population\").cast(\"float\").alias(\"total_population_sum\")\n",
    ")\n",
    "\n",
    "# Compute average income per person in each area\n",
    "result1_df = area_aggregated_df.withColumn(\n",
    "    \"average_income_per_person\",\n",
    "    (col(\"total_housholds_income\")) / col(\"total_population_sum\")\n",
    ")\n",
    "\n",
    "# Load crime data and filter invalid geographical entries\n",
    "crime_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/\")\n",
    "crime_df = crime_df.filter((col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "crime_df = crime_df.withColumn(\"geography\", ST_Point(\"LON\", \"LAT\"))\n",
    "crime_df = crime_df.select(col(\"geography\"))\n",
    "\n",
    "# Spatial join between crimes and combined population geometries\n",
    "joined2_df = crime_df.join(\n",
    "    population_geometry_combined, ST_Within(crime_df.geography, population_geometry_combined.combined_geometry)\n",
    ")\n",
    "\n",
    "# Aggregate crime counts and total population for each area and zip\n",
    "joined2_df = joined2_df.groupBy(\"area\", \"zip\").agg(\n",
    "    count(\"*\").cast(\"float\").alias(\"total_crimes\"),\n",
    "    first(\"total_population\").cast(\"float\").alias(\"total_population\")\n",
    ")\n",
    "\n",
    "# Aggregate crime data by area\n",
    "joined2_df = joined2_df.groupBy(\"area\").agg(\n",
    "    sum(col(\"total_crimes\")).cast(\"float\").alias(\"total_population_crime\"),\n",
    "    sum(\"total_population\").cast(\"float\").alias(\"total_population_sum\")\n",
    ")\n",
    "\n",
    "# Calculate crime ratio for each area\n",
    "result2_df = joined2_df.withColumn(\n",
    "    \"crime_ratio\",\n",
    "    (col(\"total_population_crime\")) / col(\"total_population_sum\")\n",
    ")\n",
    "\n",
    "# Combine crime data with average income data\n",
    "final_df = result2_df.join(\n",
    "    result1_df.select(\"area\", \"average_income_per_person\"),\n",
    "    on=\"area\",\n",
    "    how=\"inner\"\n",
    ").select(\"area\", \"crime_ratio\", \"average_income_per_person\")\n",
    "\n",
    "# Filter out rows with null or empty area values\n",
    "final_df = final_df.filter(final_df[\"area\"].isNotNull() & (trim(final_df[\"area\"]) != \"\"))\n",
    "\n",
    "# Display final results\n",
    "final_df.show()\n",
    "\n",
    "# Measure execution time for the entire DataFrame API process\n",
    "execution_time_df = time.time() - start_time_df\n",
    "print(f\"Execution time for DataFrame API: {execution_time_df} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ad16e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
