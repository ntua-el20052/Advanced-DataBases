{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea13464-ee26-43b2-a66c-7d3ac13315db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1831</td><td>application_1732639283265_1792</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1792/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-193.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1792_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"Query_2\").getOrCreate()\n",
    "\n",
    "# Load crime data from S3\n",
    "crime_data_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4668210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------+---+\n",
      "|year|   precinct|   closed_case_rate|  #|\n",
      "+----+-----------+-------------------+---+\n",
      "|2010|    Rampart|    0.3285090742017|  1|\n",
      "|2010|    Olympic| 0.3151528982199909|  2|\n",
      "|2010|     Harbor| 0.2936028339237341|  3|\n",
      "|2011|    Olympic| 0.3503192688118192|  1|\n",
      "|2011|    Rampart|0.32500296103280824|  2|\n",
      "|2011|     Harbor|0.28516260162601625|  3|\n",
      "|2012|    Olympic|0.34295435879385194|  1|\n",
      "|2012|    Rampart|  0.324610374505699|  2|\n",
      "|2012|     Harbor| 0.2953483432455395|  3|\n",
      "|2013|    Olympic| 0.3358217940999398|  1|\n",
      "|2013|    Rampart|0.32106038291605304|  2|\n",
      "|2013|     Harbor| 0.2970696405267529|  3|\n",
      "|2014|   Van Nuys|0.32002956393200294|  1|\n",
      "|2014|West Valley| 0.3151271079788573|  2|\n",
      "|2014|    Mission| 0.3121740874448456|  3|\n",
      "|2015|   Van Nuys|0.32265140677157844|  1|\n",
      "|2015|    Mission|0.30466622852314335|  2|\n",
      "|2015|   Foothill| 0.3035300180365885|  3|\n",
      "|2016|   Van Nuys|  0.321880650994575|  1|\n",
      "|2016|West Valley|0.31404702970297027|  2|\n",
      "+----+-----------+-------------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time for DataFrame API: 12.204180240631104 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, year, desc, to_timestamp, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import time  # Importing time for tracking execution duration\n",
    "\n",
    "# Start timer to measure execution time\n",
    "start_time_df = time.time()\n",
    "\n",
    "# Load the crime data into a Spark DataFrame\n",
    "crime_df = spark.read.csv(crime_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with invalid coordinates (LAT = 0 or LON = 0)\n",
    "crime_df = crime_df.filter((col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "\n",
    "# Convert the \"DATE OCC\" column to a timestamp and extract the year into a new column 'YEAR'\n",
    "crime_df = crime_df.withColumn('YEAR', to_timestamp(crime_df['DATE OCC'], 'MM/dd/yyyy hh:mm:ss a'))\n",
    "\n",
    "# Filter the dataset to include only closed cases (not under investigation or unknown)\n",
    "# Create new columns 'year' and 'precinct' for grouping\n",
    "# Group by year and precinct and calculate the count of closed cases\n",
    "closed_cases_df = crime_df.filter((crime_df['Status Desc'] != 'Invest Cont') & (crime_df['Status Desc'] != 'UNK')) \\\n",
    "    .withColumn('year', year(crime_df['YEAR'])) \\\n",
    "    .withColumn('precinct', crime_df['AREA NAME']) \\\n",
    "    .groupBy('year', 'precinct') \\\n",
    "    .agg(count('*').alias('closed_cases'))\n",
    "\n",
    "# Group the dataset by year and precinct and calculate the total number of cases\n",
    "total_cases_per_year_df = crime_df.withColumn('year', year(crime_df['YEAR'])) \\\n",
    "    .withColumn('precinct', crime_df['AREA NAME']) \\\n",
    "    .groupBy('year', 'precinct') \\\n",
    "    .agg(count('*').alias('total_cases'))\n",
    "\n",
    "# Join the closed cases DataFrame with the total cases DataFrame on year and precinct\n",
    "joined_df = closed_cases_df.join(total_cases_per_year_df, on=['year', 'precinct'])\n",
    "\n",
    "# Calculate the closed case rate for each year and precinct\n",
    "joined_df = joined_df.withColumn('closed_case_rate', col('closed_cases') / col('total_cases'))\n",
    "\n",
    "# Define a window specification to rank precincts by closed case rate within each year\n",
    "window_spec = Window.partitionBy('year').orderBy(F.desc('closed_case_rate'))\n",
    "\n",
    "# Add a rank column to the DataFrame based on the closed case rate\n",
    "ranked_df = joined_df.withColumn('#', F.rank().over(window_spec))\n",
    "\n",
    "# Filter the DataFrame to include only the top 3 precincts by closed case rate for each year\n",
    "# Select relevant columns for the final output\n",
    "top_3_departments_df = ranked_df.filter(ranked_df['#'] <= 3).select('year', 'precinct', 'closed_case_rate', '#')\n",
    "\n",
    "# Display the top 3 precincts for each year\n",
    "top_3_departments_df.show()\n",
    "\n",
    "# Calculate and print the execution time for the DataFrame API operations\n",
    "execution_time_df = time.time() - start_time_df\n",
    "print(f\"Execution time for DataFrame API: {execution_time_df} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "670c1cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------------+---+\n",
      "|year|   precinct|  closed_case_rate|  #|\n",
      "+----+-----------+------------------+---+\n",
      "|2010|    Rampart|0.3285090742017000|  1|\n",
      "|2010|    Olympic|0.3151528982199909|  2|\n",
      "|2010|     Harbor|0.2936028339237341|  3|\n",
      "|2011|    Olympic|0.3503192688118192|  1|\n",
      "|2011|    Rampart|0.3250029610328082|  2|\n",
      "|2011|     Harbor|0.2851626016260163|  3|\n",
      "|2012|    Olympic|0.3429543587938519|  1|\n",
      "|2012|    Rampart|0.3246103745056990|  2|\n",
      "|2012|     Harbor|0.2953483432455395|  3|\n",
      "|2013|    Olympic|0.3358217940999398|  1|\n",
      "|2013|    Rampart|0.3210603829160530|  2|\n",
      "|2013|     Harbor|0.2970696405267529|  3|\n",
      "|2014|   Van Nuys|0.3200295639320030|  1|\n",
      "|2014|West Valley|0.3151271079788573|  2|\n",
      "|2014|    Mission|0.3121740874448456|  3|\n",
      "|2015|   Van Nuys|0.3226514067715784|  1|\n",
      "|2015|    Mission|0.3046662285231434|  2|\n",
      "|2015|   Foothill|0.3035300180365885|  3|\n",
      "|2016|   Van Nuys|0.3218806509945750|  1|\n",
      "|2016|West Valley|0.3140470297029703|  2|\n",
      "+----+-----------+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time for SQL API: 8.220915079116821 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from pyspark.sql.functions import count, year, desc, to_timestamp, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Start a timer to measure execution time\n",
    "start_time_sql = time.time()\n",
    "\n",
    "# Load crime data from the specified path into a DataFrame\n",
    "crime1 = spark.read.csv(crime_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Filter out invalid geographical entries (where LAT or LON is 0)\n",
    "crime = crime1.filter((col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "\n",
    "# Convert the 'DATE OCC' column to a timestamp and extract the 'YEAR' field\n",
    "crime = crime.withColumn('YEAR', to_timestamp(crime['DATE OCC'], 'MM/dd/yyyy hh:mm:ss a'))\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table for query execution\n",
    "crime.createOrReplaceTempView('crime_data')\n",
    "\n",
    "# SQL Query: Analyze the top 3 precincts per year based on closed case rates\n",
    "sql_query = \"\"\"\n",
    "WITH aggregated_data AS (\n",
    "    SELECT \n",
    "        YEAR(YEAR) AS year,  -- Extract the year from the timestamp\n",
    "        `AREA NAME` AS precinct,  -- Use precinct name\n",
    "        SUM(CASE WHEN `Status Desc` NOT IN ('Invest Cont', 'UNK') THEN 1 ELSE 0 END) AS closed_cases, -- Count closed cases\n",
    "        COUNT(*) AS total_cases  -- Count total cases\n",
    "    FROM crime_data\n",
    "    GROUP BY YEAR(YEAR), `AREA NAME`  -- Group data by year and precinct\n",
    "),\n",
    "ranked_data AS (\n",
    "    SELECT \n",
    "        year, \n",
    "        precinct, \n",
    "        closed_cases * 1.0 / total_cases AS closed_case_rate,  -- Calculate closure rate\n",
    "        RANK() OVER (PARTITION BY year ORDER BY closed_cases * 1.0 / total_cases DESC) AS rank -- Rank precincts by closure rate\n",
    "    FROM aggregated_data\n",
    ")\n",
    "SELECT \n",
    "    year, \n",
    "    precinct, \n",
    "    closed_case_rate, \n",
    "    rank AS `#`\n",
    "FROM ranked_data\n",
    "WHERE rank <= 3 -- Filter top 3 precincts per year\n",
    "ORDER BY year, `#`; -- Sort by year and rank\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query and store the results in a DataFrame\n",
    "top_3_departments_sql = spark.sql(sql_query)\n",
    "\n",
    "# Display the top 3 precincts per year along with their closure rate and rank\n",
    "top_3_departments_sql.show()\n",
    "\n",
    "# Measure execution time for the entire process\n",
    "execution_time_sql = time.time() - start_time_sql\n",
    "print(f\"Execution time for SQL API: {execution_time_sql} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf372a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3_bucket = \"s3://groups-bucket-dblab-905418150721/group19/datasheet.parquet\"\n",
    "\n",
    "crime_data_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/\"\n",
    "crime_df = spark.read.csv(crime_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Coalesce the data into a single partition to create a single .parquet file\n",
    "single_file_df = crime_df.coalesce(1)\n",
    "\n",
    "# Save the DataFrame as a Parquet file to the specified S3 bucket\n",
    "single_file_df.write.mode(\"overwrite\").parquet(s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e05a9a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------+---+\n",
      "|year|   precinct|   closed_case_rate|  #|\n",
      "+----+-----------+-------------------+---+\n",
      "|2010|    Rampart|    0.3285090742017|  1|\n",
      "|2010|    Olympic| 0.3151528982199909|  2|\n",
      "|2010|     Harbor| 0.2936028339237341|  3|\n",
      "|2011|    Olympic| 0.3503192688118192|  1|\n",
      "|2011|    Rampart|0.32500296103280824|  2|\n",
      "|2011|     Harbor|0.28516260162601625|  3|\n",
      "|2012|    Olympic|0.34295435879385194|  1|\n",
      "|2012|    Rampart|  0.324610374505699|  2|\n",
      "|2012|     Harbor| 0.2953483432455395|  3|\n",
      "|2013|    Olympic| 0.3358217940999398|  1|\n",
      "|2013|    Rampart|0.32106038291605304|  2|\n",
      "|2013|     Harbor| 0.2970696405267529|  3|\n",
      "|2014|   Van Nuys|0.32002956393200294|  1|\n",
      "|2014|West Valley| 0.3151271079788573|  2|\n",
      "|2014|    Mission| 0.3121740874448456|  3|\n",
      "|2015|   Van Nuys|0.32265140677157844|  1|\n",
      "|2015|    Mission|0.30466622852314335|  2|\n",
      "|2015|   Foothill| 0.3035300180365885|  3|\n",
      "|2016|   Van Nuys|  0.321880650994575|  1|\n",
      "|2016|West Valley|0.31404702970297027|  2|\n",
      "+----+-----------+-------------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time for DataFrame API: 26.191636085510254 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, year, desc, to_timestamp, col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import time  # Importing the time module to track execution duration\n",
    "\n",
    "# Start timer to measure execution time\n",
    "start_time_df = time.time()\n",
    "\n",
    "# Convert the 'DATE OCC' column to a timestamp format and add a new column 'YEAR'\n",
    "single_file_df = single_file_df.withColumn('YEAR', to_timestamp(single_file_df['DATE OCC'], 'MM/dd/yyyy hh:mm:ss a'))\n",
    "\n",
    "# Filter out records with invalid latitude and longitude (assumed as (LAT = 0) or (LON = 0))\n",
    "single_file_df = single_file_df.filter((col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "\n",
    "# Filter to include only closed cases (excluding \"Invest Cont\" and \"UNK\"), then calculate the count of closed cases\n",
    "# Group by year and precinct, and create new columns 'year' and 'precinct'\n",
    "closed_cases_df = single_file_df.filter((single_file_df['Status Desc'] != 'Invest Cont') & \n",
    "                                        (single_file_df['Status Desc'] != 'UNK')) \\\n",
    "    .withColumn('year', year(single_file_df['YEAR'])) \\\n",
    "    .withColumn('precinct', single_file_df['AREA NAME']) \\\n",
    "    .groupBy('year', 'precinct') \\\n",
    "    .agg(count('*').alias('closed_cases'))\n",
    "\n",
    "# Calculate the total cases per year and precinct\n",
    "total_cases_per_year_df = single_file_df.withColumn('year', year(single_file_df['YEAR'])) \\\n",
    "    .withColumn('precinct', single_file_df['AREA NAME']) \\\n",
    "    .groupBy('year', 'precinct') \\\n",
    "    .agg(count('*').alias('total_cases'))\n",
    "\n",
    "# Join the DataFrame of closed cases with the total cases DataFrame on 'year' and 'precinct'\n",
    "joined_df = closed_cases_df.join(total_cases_per_year_df, on=['year', 'precinct'])\n",
    "\n",
    "# Add a new column for the closed case rate (ratio of closed cases to total cases)\n",
    "joined_df = joined_df.withColumn('closed_case_rate', col('closed_cases') / col('total_cases'))\n",
    "\n",
    "# Define a window specification to rank precincts by their closed case rate for each year\n",
    "window_spec = Window.partitionBy('year').orderBy(F.desc('closed_case_rate'))\n",
    "\n",
    "# Add a rank column to the DataFrame based on the closed case rate\n",
    "ranked_df = joined_df.withColumn('#', F.rank().over(window_spec))\n",
    "\n",
    "# Filter the DataFrame to include only the top 3 precincts for each year\n",
    "# Select the relevant columns for the final output\n",
    "top_3_departments_df = ranked_df.filter(ranked_df['#'] <= 3).select('year', 'precinct', 'closed_case_rate', '#')\n",
    "\n",
    "# Display the top 3 precincts for each year along with their closed case rates\n",
    "top_3_departments_df.show()\n",
    "\n",
    "# Calculate and print the total execution time for the DataFrame API operations\n",
    "execution_time_df = time.time() - start_time_df\n",
    "print(f\"Execution time for DataFrame API: {execution_time_df} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb8b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
