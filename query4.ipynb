{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea13464-ee26-43b2-a66c-7d3ac13315db",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2023</td><td>application_1732639283265_1983</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_1983/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-112.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_1983_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Income Areas:\n",
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White| 57|\n",
      "|               Other| 15|\n",
      "|Hispanic/Latin/Me...|  8|\n",
      "|             Unknown|  8|\n",
      "|               Black|  5|\n",
      "|         Other Asian|  2|\n",
      "|                NULL|  0|\n",
      "+--------------------+---+\n",
      "\n",
      "Low-Income Areas:\n",
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|470|\n",
      "|               Other|128|\n",
      "|Hispanic/Latin/Me...|127|\n",
      "|             Unknown| 90|\n",
      "|               Black| 56|\n",
      "|         Other Asian| 24|\n",
      "|            Filipino|  2|\n",
      "|            Japanese|  1|\n",
      "|                NULL|  0|\n",
      "+--------------------+---+\n",
      "\n",
      "Time taken: 48.78 seconds"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 - Racial Profile Analysis\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load GeoJSON data for Census Blocks\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = spark.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "# Load datasets\n",
    "crime_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "income_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "race_codes = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True) \\\n",
    "            .withColumnRenamed(\"Vict Descent\", \"Race_Code\") \\\n",
    "            .withColumnRenamed(\"Vict Descent Full\", \"Race_Description\")\n",
    "\n",
    "# Filter crime data for 2015\n",
    "crime_2015 = crime_data.filter(col(\"DATE OCC\").contains(\"2015\"))\n",
    "\n",
    "# Remove invalid records (Null Island)\n",
    "crime_2015 = crime_2015.filter((col(\"LAT\").isNotNull()) & (col(\"LON\").isNotNull()) & (col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "\n",
    "# Add geometry column to crime data\n",
    "crime_2015 = crime_2015.withColumn(\"geometry\", ST_Point(col(\"LON\"), col(\"LAT\")))\n",
    "\n",
    "# Identify the 3 ZIP codes with highest and lowest income\n",
    "high_income_zips = income_data.orderBy(desc(\"Estimated Median Income\")).limit(3)\n",
    "low_income_zips = income_data.orderBy(\"Estimated Median Income\").limit(3)\n",
    "\n",
    "# Broadcast high-income and low-income ZIP codes\n",
    "high_income_zip_list = [row[\"Zip Code\"] for row in high_income_zips.collect()]\n",
    "low_income_zip_list = [row[\"Zip Code\"] for row in low_income_zips.collect()]\n",
    "\n",
    "# Filter census blocks for high- and low-income areas\n",
    "high_income_blocks = flattened_df.filter(flattened_df[\"ZCTA10\"].isin(high_income_zip_list))\n",
    "low_income_blocks = flattened_df.filter(flattened_df[\"ZCTA10\"].isin(low_income_zip_list))\n",
    "\n",
    "high_income_crimes = crime_2015.join(\n",
    "    high_income_blocks,\n",
    "    ST_Within(crime_2015.geometry, high_income_blocks.geometry),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "low_income_crimes = crime_2015.join(\n",
    "    low_income_blocks,\n",
    "    ST_Within(crime_2015.geometry, low_income_blocks.geometry),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Map racial profile codes to full descriptions\n",
    "high_income_racial_profile = high_income_crimes \\\n",
    "    .join(race_codes, high_income_crimes[\"Vict Descent\"] == race_codes[\"Race_Code\"], \"left\") \\\n",
    "    .groupBy(\"Race_Description\") \\\n",
    "    .agg(count(\"Vict Descent\").alias(\"Victim_Count\")) \\\n",
    "    .orderBy(desc(\"Victim_Count\")) \\\n",
    "    .withColumnRenamed(\"Race_Description\", \"Victim Descent\") \\\n",
    "    .withColumnRenamed(\"Victim_Count\", \"#\")\n",
    "\n",
    "low_income_racial_profile = low_income_crimes \\\n",
    "    .join(race_codes, low_income_crimes[\"Vict Descent\"] == race_codes[\"Race_Code\"], \"left\") \\\n",
    "    .groupBy(\"Race_Description\") \\\n",
    "    .agg(count(\"Vict Descent\").alias(\"Victim_Count\")) \\\n",
    "    .orderBy(desc(\"Victim_Count\")) \\\n",
    "    .withColumnRenamed(\"Race_Description\", \"Victim Descent\") \\\n",
    "    .withColumnRenamed(\"Victim_Count\", \"#\")\n",
    "\n",
    "# Show results\n",
    "print(\"High-Income Areas:\")\n",
    "high_income_racial_profile.show()\n",
    "\n",
    "print(\"Low-Income Areas:\")\n",
    "low_income_racial_profile.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c96b23-522e-4304-b752-9404e6f46c00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Income Areas:\n",
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White| 57|\n",
      "|               Other| 15|\n",
      "|Hispanic/Latin/Me...|  8|\n",
      "|             Unknown|  8|\n",
      "|               Black|  5|\n",
      "|         Other Asian|  2|\n",
      "|                NULL|  0|\n",
      "+--------------------+---+\n",
      "\n",
      "Low-Income Areas:\n",
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|470|\n",
      "|               Other|128|\n",
      "|Hispanic/Latin/Me...|127|\n",
      "|             Unknown| 90|\n",
      "|               Black| 56|\n",
      "|         Other Asian| 24|\n",
      "|            Filipino|  2|\n",
      "|            Japanese|  1|\n",
      "|                NULL|  0|\n",
      "+--------------------+---+\n",
      "\n",
      "Time taken: 32.18 seconds"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 - Racial Profile Analysis\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load GeoJSON data for Census Blocks\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = spark.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "# Load datasets\n",
    "crime_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "income_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "race_codes = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True) \\\n",
    "            .withColumnRenamed(\"Vict Descent\", \"Race_Code\") \\\n",
    "            .withColumnRenamed(\"Vict Descent Full\", \"Race_Description\")\n",
    "\n",
    "# Filter crime data for 2015\n",
    "crime_2015 = crime_data.filter(col(\"DATE OCC\").contains(\"2015\"))\n",
    "\n",
    "# Remove invalid records (Null Island)\n",
    "crime_2015 = crime_2015.filter((col(\"LAT\").isNotNull()) & (col(\"LON\").isNotNull()) & (col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "\n",
    "# Add geometry column to crime data\n",
    "crime_2015 = crime_2015.withColumn(\"geometry\", ST_Point(col(\"LON\"), col(\"LAT\")))\n",
    "\n",
    "# Identify the 3 ZIP codes with highest and lowest income\n",
    "high_income_zips = income_data.orderBy(desc(\"Estimated Median Income\")).limit(3)\n",
    "low_income_zips = income_data.orderBy(\"Estimated Median Income\").limit(3)\n",
    "\n",
    "# Broadcast high-income and low-income ZIP codes\n",
    "high_income_zip_list = [row[\"Zip Code\"] for row in high_income_zips.collect()]\n",
    "low_income_zip_list = [row[\"Zip Code\"] for row in low_income_zips.collect()]\n",
    "\n",
    "# Filter census blocks for high- and low-income areas\n",
    "high_income_blocks = flattened_df.filter(flattened_df[\"ZCTA10\"].isin(high_income_zip_list))\n",
    "low_income_blocks = flattened_df.filter(flattened_df[\"ZCTA10\"].isin(low_income_zip_list))\n",
    "\n",
    "high_income_crimes = crime_2015.join(\n",
    "    high_income_blocks,\n",
    "    ST_Within(crime_2015.geometry, high_income_blocks.geometry),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "low_income_crimes = crime_2015.join(\n",
    "    low_income_blocks,\n",
    "    ST_Within(crime_2015.geometry, low_income_blocks.geometry),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Map racial profile codes to full descriptions\n",
    "high_income_racial_profile = high_income_crimes \\\n",
    "    .join(race_codes, high_income_crimes[\"Vict Descent\"] == race_codes[\"Race_Code\"], \"left\") \\\n",
    "    .groupBy(\"Race_Description\") \\\n",
    "    .agg(count(\"Vict Descent\").alias(\"Victim_Count\")) \\\n",
    "    .orderBy(desc(\"Victim_Count\")) \\\n",
    "    .withColumnRenamed(\"Race_Description\", \"Victim Descent\") \\\n",
    "    .withColumnRenamed(\"Victim_Count\", \"#\")\n",
    "\n",
    "low_income_racial_profile = low_income_crimes \\\n",
    "    .join(race_codes, low_income_crimes[\"Vict Descent\"] == race_codes[\"Race_Code\"], \"left\") \\\n",
    "    .groupBy(\"Race_Description\") \\\n",
    "    .agg(count(\"Vict Descent\").alias(\"Victim_Count\")) \\\n",
    "    .orderBy(desc(\"Victim_Count\")) \\\n",
    "    .withColumnRenamed(\"Race_Description\", \"Victim Descent\") \\\n",
    "    .withColumnRenamed(\"Victim_Count\", \"#\")\n",
    "\n",
    "# Show results\n",
    "print(\"High-Income Areas:\")\n",
    "high_income_racial_profile.show()\n",
    "\n",
    "print(\"Low-Income Areas:\")\n",
    "low_income_racial_profile.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3b8111-3cc0-4b82-a432-edddc869eee6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Income Areas:\n",
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White| 57|\n",
      "|               Other| 15|\n",
      "|Hispanic/Latin/Me...|  8|\n",
      "|             Unknown|  8|\n",
      "|               Black|  5|\n",
      "|         Other Asian|  2|\n",
      "|                NULL|  0|\n",
      "+--------------------+---+\n",
      "\n",
      "Low-Income Areas:\n",
      "+--------------------+---+\n",
      "|      Victim Descent|  #|\n",
      "+--------------------+---+\n",
      "|               White|470|\n",
      "|               Other|128|\n",
      "|Hispanic/Latin/Me...|127|\n",
      "|             Unknown| 90|\n",
      "|               Black| 56|\n",
      "|         Other Asian| 24|\n",
      "|            Filipino|  2|\n",
      "|            Japanese|  1|\n",
      "|                NULL|  0|\n",
      "+--------------------+---+\n",
      "\n",
      "Time taken: 29.11 seconds"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 4 - Racial Profile Analysis\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Sedona context\n",
    "sedona = SedonaContext.create(spark)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load GeoJSON data for Census Blocks\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "blocks_df = spark.read.format(\"geojson\") \\\n",
    "            .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "            .selectExpr(\"explode(features) as features\") \\\n",
    "            .select(\"features.*\")\n",
    "\n",
    "# Formatting magic\n",
    "flattened_df = blocks_df.select( \\\n",
    "                [col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    "            .drop(\"properties\") \\\n",
    "            .drop(\"type\")\n",
    "\n",
    "# Load datasets\n",
    "crime_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "income_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "race_codes = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True) \\\n",
    "            .withColumnRenamed(\"Vict Descent\", \"Race_Code\") \\\n",
    "            .withColumnRenamed(\"Vict Descent Full\", \"Race_Description\")\n",
    "\n",
    "# Filter crime data for 2015\n",
    "crime_2015 = crime_data.filter(col(\"DATE OCC\").contains(\"2015\"))\n",
    "\n",
    "# Remove invalid records (Null Island)\n",
    "crime_2015 = crime_2015.filter((col(\"LAT\").isNotNull()) & (col(\"LON\").isNotNull()) & (col(\"LAT\") != 0) & (col(\"LON\") != 0))\n",
    "\n",
    "# Add geometry column to crime data\n",
    "crime_2015 = crime_2015.withColumn(\"geometry\", ST_Point(col(\"LON\"), col(\"LAT\")))\n",
    "\n",
    "# Identify the 3 ZIP codes with highest and lowest income\n",
    "high_income_zips = income_data.orderBy(desc(\"Estimated Median Income\")).limit(3)\n",
    "low_income_zips = income_data.orderBy(\"Estimated Median Income\").limit(3)\n",
    "\n",
    "# Broadcast high-income and low-income ZIP codes\n",
    "high_income_zip_list = [row[\"Zip Code\"] for row in high_income_zips.collect()]\n",
    "low_income_zip_list = [row[\"Zip Code\"] for row in low_income_zips.collect()]\n",
    "\n",
    "# Filter census blocks for high- and low-income areas\n",
    "high_income_blocks = flattened_df.filter(flattened_df[\"ZCTA10\"].isin(high_income_zip_list))\n",
    "low_income_blocks = flattened_df.filter(flattened_df[\"ZCTA10\"].isin(low_income_zip_list))\n",
    "\n",
    "high_income_crimes = crime_2015.join(\n",
    "    high_income_blocks,\n",
    "    ST_Within(crime_2015.geometry, high_income_blocks.geometry),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "low_income_crimes = crime_2015.join(\n",
    "    low_income_blocks,\n",
    "    ST_Within(crime_2015.geometry, low_income_blocks.geometry),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Map racial profile codes to full descriptions\n",
    "high_income_racial_profile = high_income_crimes \\\n",
    "    .join(race_codes, high_income_crimes[\"Vict Descent\"] == race_codes[\"Race_Code\"], \"left\") \\\n",
    "    .groupBy(\"Race_Description\") \\\n",
    "    .agg(count(\"Vict Descent\").alias(\"Victim_Count\")) \\\n",
    "    .orderBy(desc(\"Victim_Count\")) \\\n",
    "    .withColumnRenamed(\"Race_Description\", \"Victim Descent\") \\\n",
    "    .withColumnRenamed(\"Victim_Count\", \"#\")\n",
    "\n",
    "low_income_racial_profile = low_income_crimes \\\n",
    "    .join(race_codes, low_income_crimes[\"Vict Descent\"] == race_codes[\"Race_Code\"], \"left\") \\\n",
    "    .groupBy(\"Race_Description\") \\\n",
    "    .agg(count(\"Vict Descent\").alias(\"Victim_Count\")) \\\n",
    "    .orderBy(desc(\"Victim_Count\")) \\\n",
    "    .withColumnRenamed(\"Race_Description\", \"Victim Descent\") \\\n",
    "    .withColumnRenamed(\"Victim_Count\", \"#\")\n",
    "\n",
    "# Show results\n",
    "print(\"High-Income Areas:\")\n",
    "high_income_racial_profile.show()\n",
    "\n",
    "print(\"Low-Income Areas:\")\n",
    "low_income_racial_profile.show()\n",
    "\n",
    "# Stop timing and print out the execution duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Stop Spark session\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92743ac3-7fbc-4512-be7a-1d89008f0142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
